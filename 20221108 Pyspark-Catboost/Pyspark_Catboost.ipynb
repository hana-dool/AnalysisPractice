{"cells":[{"cell_type":"markdown","metadata":{"id":"zRus8daK-ebJ"},"source":["# Pyspark - Catboost\n","- Pyspark 를 사용해서 Catboost 사용하는 Example\n","- Colab 에서 작동합니다."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19742,"status":"ok","timestamp":1669145309767,"user":{"displayName":"rany go","userId":"15846384859939162308"},"user_tz":-540},"id":"JM8ng9tJ-AG6","outputId":"e8318012-d783-4893-8972-dd62b1d817e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# 먼저 Google-colab 에 mount 를 시킨다. \n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"4fGC5NvW-LhH","executionInfo":{"status":"ok","timestamp":1669145321687,"user_tz":-540,"elapsed":11926,"user":{"displayName":"rany go","userId":"15846384859939162308"}}},"outputs":[],"source":["# java 설치\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null "]},{"cell_type":"code","execution_count":3,"metadata":{"id":"_3v0Rcgq-MT3","executionInfo":{"status":"ok","timestamp":1669145325891,"user_tz":-540,"elapsed":4206,"user":{"displayName":"rany go","userId":"15846384859939162308"}}},"outputs":[],"source":["# spark 설치\n","!wget -q https://dlcdn.apache.org/spark/spark-3.2.2/spark-3.2.2-bin-hadoop2.7.tgz"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"RDwU9cU1-NCi","executionInfo":{"status":"ok","timestamp":1669145328844,"user_tz":-540,"elapsed":2958,"user":{"displayName":"rany go","userId":"15846384859939162308"}}},"outputs":[],"source":["# 설치 파일 압축 풀기\n","!tar xf spark-3.2.2-bin-hadoop2.7.tgz"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11061,"status":"ok","timestamp":1669145339899,"user":{"displayName":"rany go","userId":"15846384859939162308"},"user_tz":-540},"id":"p6XK0VKm-Nww","outputId":"4418ec78-f14d-4775-f150-ad696a7d47f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 76.3 MB 1.2 MB/s \n","\u001b[?25h"]}],"source":["# catboost 와 findsaprk 설치 \n","!pip install -q findspark==1.4.2 catboost==1.0.3"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"yyG77gy1-Ow3","executionInfo":{"status":"ok","timestamp":1669145733138,"user_tz":-540,"elapsed":439,"user":{"displayName":"rany go","userId":"15846384859939162308"}}},"outputs":[],"source":["# JAVA_Home 과 SPARK_HOME 세팅\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.2-bin-hadoop2.7\"\n","import findspark\n","findspark.init()"]},{"cell_type":"markdown","source":["# spark - submit 사용"],"metadata":{"id":"cSmJJ1R72UfQ"}},{"cell_type":"code","source":["%%writefile example.py\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"appName\").getOrCreate()\n","sc = spark.sparkContext\n","rdd = sc.parallelize([1,2,3,4,5,6,7])\n","print(rdd.count())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4jm5vG8u1H3c","executionInfo":{"status":"ok","timestamp":1669145604186,"user_tz":-540,"elapsed":2,"user":{"displayName":"rany go","userId":"15846384859939162308"}},"outputId":"b98d6e09-838d-465c-9c68-f518c43b84b6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing example.py\n"]}]},{"cell_type":"code","source":["bin/spark-submit /content/example.py"],"metadata":{"id":"YUG9p9AF2XRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%run -i example.py"],"metadata":{"id":"npRoflQ-2ZPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!bash"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z7Y5iKpG0TuZ","executionInfo":{"status":"ok","timestamp":1669145954431,"user_tz":-540,"elapsed":194134,"user":{"displayName":"rany go","userId":"15846384859939162308"}},"outputId":"c0dc1394-d3a8-4e14-bdd8-f4f32bb73f6a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["bash: cannot set terminal process group (538): Inappropriate ioctl for device\n","bash: no job control in this shell\n","\u001b[01;34m/content\u001b[00m# ls\n","\u001b[0m\u001b[01;34mdrive\u001b[0m       \u001b[01;34msample_data\u001b[0m                \u001b[01;31mspark-3.2.2-bin-hadoop2.7.tgz\u001b[0m\n","example.py  \u001b[01;34mspark-3.2.2-bin-hadoop2.7\u001b[0m\n","\u001b[01;34m/content\u001b[00m# cd spark-3.2.2-bin-hadoop2.7\n","\u001b[01;34m/content/spark-3.2.2-bin-hadoop2.7\u001b[00m# ls\n","\u001b[0m\u001b[01;34mbin\u001b[0m   \u001b[01;34mdata\u001b[0m      \u001b[01;34mjars\u001b[0m        LICENSE   NOTICE  \u001b[01;34mR\u001b[0m          RELEASE  \u001b[01;34myarn\u001b[0m\n","\u001b[01;34mconf\u001b[0m  \u001b[01;34mexamples\u001b[0m  \u001b[01;34mkubernetes\u001b[0m  \u001b[01;34mlicenses\u001b[0m  \u001b[01;34mpython\u001b[0m  README.md  \u001b[01;34msbin\u001b[0m\n","\u001b[01;34m/content/spark-3.2.2-bin-hadoop2.7\u001b[00m# bin/spark-submit\n","Usage: spark-submit [options] <app jar | python file | R file> [app arguments]\n","Usage: spark-submit --kill [submission ID] --master [spark://...]\n","Usage: spark-submit --status [submission ID] --master [spark://...]\n","Usage: spark-submit run-example [options] example-class [example args]\n","\n","Options:\n","  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n","                              k8s://https://host:port, or local (Default: local[*]).\n","  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n","                              on one of the worker machines inside the cluster (\"cluster\")\n","                              (Default: client).\n","  --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n","  --name NAME                 A name of your application.\n","  --jars JARS                 Comma-separated list of jars to include on the driver\n","                              and executor classpaths.\n","  --packages                  Comma-separated list of maven coordinates of jars to include\n","                              on the driver and executor classpaths. Will search the local\n","                              maven repo, then maven central and any additional remote\n","                              repositories given by --repositories. The format for the\n","                              coordinates should be groupId:artifactId:version.\n","  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n","                              resolving the dependencies provided in --packages to avoid\n","                              dependency conflicts.\n","  --repositories              Comma-separated list of additional remote repositories to\n","                              search for the maven coordinates given with --packages.\n","  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n","                              on the PYTHONPATH for Python apps.\n","  --files FILES               Comma-separated list of files to be placed in the working\n","                              directory of each executor. File paths of these files\n","                              in executors can be accessed via SparkFiles.get(fileName).\n","  --archives ARCHIVES         Comma-separated list of archives to be extracted into the\n","                              working directory of each executor.\n","\n","  --conf, -c PROP=VALUE       Arbitrary Spark configuration property.\n","  --properties-file FILE      Path to a file from which to load extra properties. If not\n","                              specified, this will look for conf/spark-defaults.conf.\n","\n","  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n","  --driver-java-options       Extra Java options to pass to the driver.\n","  --driver-library-path       Extra library path entries to pass to the driver.\n","  --driver-class-path         Extra class path entries to pass to the driver. Note that\n","                              jars added with --jars are automatically included in the\n","                              classpath.\n","\n","  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n","\n","  --proxy-user NAME           User to impersonate when submitting the application.\n","                              This argument does not work with --principal / --keytab.\n","\n","  --help, -h                  Show this help message and exit.\n","  --verbose, -v               Print additional debug output.\n","  --version,                  Print the version of current Spark.\n","\n"," Cluster deploy mode only:\n","  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n","                              (Default: 1).\n","\n"," Spark standalone or Mesos with cluster deploy mode only:\n","  --supervise                 If given, restarts the driver on failure.\n","\n"," Spark standalone, Mesos or K8s with cluster deploy mode only:\n","  --kill SUBMISSION_ID        If given, kills the driver specified.\n","  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n","\n"," Spark standalone, Mesos and Kubernetes only:\n","  --total-executor-cores NUM  Total cores for all executors.\n","\n"," Spark standalone, YARN and Kubernetes only:\n","  --executor-cores NUM        Number of cores used by each executor. (Default: 1 in\n","                              YARN and K8S modes, or all available cores on the worker\n","                              in standalone mode).\n","\n"," Spark on YARN and Kubernetes only:\n","  --num-executors NUM         Number of executors to launch (Default: 2).\n","                              If dynamic allocation is enabled, the initial number of\n","                              executors will be at least NUM.\n","  --principal PRINCIPAL       Principal to be used to login to KDC.\n","  --keytab KEYTAB             The full path to the file that contains the keytab for the\n","                              principal specified above.\n","\n"," Spark on YARN only:\n","  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n","      \n","\u001b[01;34m/content/spark-3.2.2-bin-hadoop2.7\u001b[00m# bin/spark-submit content/example.py\n","/usr/bin/python3: can't open file '/content/spark-3.2.2-bin-hadoop2.7/content/example.py': [Errno 2] No such file or directory\n","log4j:WARN No appenders could be found for logger (org.apache.spark.util.ShutdownHookManager).\n","log4j:WARN Please initialize the log4j system properly.\n","log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n","\u001b[01;34m/content/spark-3.2.2-bin-hadoop2.7\u001b[00m# bin/spark-submit /content/example.py\n","Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n","22/11/22 19:37:25 INFO SparkContext: Running Spark version 3.2.2\n","22/11/22 19:37:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","22/11/22 19:37:25 INFO ResourceUtils: ==============================================================\n","22/11/22 19:37:25 INFO ResourceUtils: No custom resources configured for spark.driver.\n","22/11/22 19:37:25 INFO ResourceUtils: ==============================================================\n","22/11/22 19:37:25 INFO SparkContext: Submitted application: appName\n","22/11/22 19:37:25 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n","22/11/22 19:37:25 INFO ResourceProfile: Limiting resource is cpu\n","22/11/22 19:37:25 INFO ResourceProfileManager: Added ResourceProfile id: 0\n","22/11/22 19:37:25 INFO SecurityManager: Changing view acls to: root\n","22/11/22 19:37:25 INFO SecurityManager: Changing modify acls to: root\n","22/11/22 19:37:25 INFO SecurityManager: Changing view acls groups to: \n","22/11/22 19:37:25 INFO SecurityManager: Changing modify acls groups to: \n","22/11/22 19:37:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n","22/11/22 19:37:26 INFO Utils: Successfully started service 'sparkDriver' on port 41059.\n","22/11/22 19:37:26 INFO SparkEnv: Registering MapOutputTracker\n","22/11/22 19:37:26 INFO SparkEnv: Registering BlockManagerMaster\n","22/11/22 19:37:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n","22/11/22 19:37:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n","22/11/22 19:37:26 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","22/11/22 19:37:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3042d00b-41a2-4557-9e8e-1f81556dfeb8\n","22/11/22 19:37:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n","22/11/22 19:37:26 INFO SparkEnv: Registering OutputCommitCoordinator\n","22/11/22 19:37:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n","22/11/22 19:37:26 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n","22/11/22 19:37:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://674afbf15f88:4041\n","22/11/22 19:37:27 INFO Executor: Starting executor ID driver on host 674afbf15f88\n","22/11/22 19:37:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42245.\n","22/11/22 19:37:27 INFO NettyBlockTransferService: Server created on 674afbf15f88:42245\n","22/11/22 19:37:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n","22/11/22 19:37:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 674afbf15f88, 42245, None)\n","22/11/22 19:37:27 INFO BlockManagerMasterEndpoint: Registering block manager 674afbf15f88:42245 with 366.3 MiB RAM, BlockManagerId(driver, 674afbf15f88, 42245, None)\n","22/11/22 19:37:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 674afbf15f88, 42245, None)\n","22/11/22 19:37:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 674afbf15f88, 42245, None)\n","22/11/22 19:37:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n","22/11/22 19:37:28 INFO SharedState: Warehouse path is 'file:/content/spark-3.2.2-bin-hadoop2.7/spark-warehouse'.\n","22/11/22 19:37:30 INFO SparkContext: Starting job: count at /content/example.py:5\n","22/11/22 19:37:30 INFO DAGScheduler: Got job 0 (count at /content/example.py:5) with 2 output partitions\n","22/11/22 19:37:30 INFO DAGScheduler: Final stage: ResultStage 0 (count at /content/example.py:5)\n","22/11/22 19:37:30 INFO DAGScheduler: Parents of final stage: List()\n","22/11/22 19:37:30 INFO DAGScheduler: Missing parents: List()\n","22/11/22 19:37:30 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at count at /content/example.py:5), which has no missing parents\n","22/11/22 19:37:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 6.8 KiB, free 366.3 MiB)\n","22/11/22 19:37:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 366.3 MiB)\n","22/11/22 19:37:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 674afbf15f88:42245 (size: 4.2 KiB, free: 366.3 MiB)\n","22/11/22 19:37:30 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1478\n","22/11/22 19:37:30 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at count at /content/example.py:5) (first 15 tasks are for partitions Vector(0, 1))\n","22/11/22 19:37:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n","22/11/22 19:37:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (674afbf15f88, executor driver, partition 0, PROCESS_LOCAL, 4465 bytes) taskResourceAssignments Map()\n","22/11/22 19:37:31 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (674afbf15f88, executor driver, partition 1, PROCESS_LOCAL, 4492 bytes) taskResourceAssignments Map()\n","22/11/22 19:37:31 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n","22/11/22 19:37:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n","22/11/22 19:37:32 INFO PythonRunner: Times: total = 485, boot = 467, init = 18, finish = 0\n","22/11/22 19:37:32 INFO PythonRunner: Times: total = 519, boot = 474, init = 44, finish = 1\n","22/11/22 19:37:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1311 bytes result sent to driver\n","22/11/22 19:37:32 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1311 bytes result sent to driver\n","22/11/22 19:37:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1198 ms on 674afbf15f88 (executor driver) (1/2)\n","22/11/22 19:37:32 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50567\n","22/11/22 19:37:32 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1168 ms on 674afbf15f88 (executor driver) (2/2)\n","22/11/22 19:37:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n","22/11/22 19:37:32 INFO DAGScheduler: ResultStage 0 (count at /content/example.py:5) finished in 1.588 s\n","22/11/22 19:37:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n","22/11/22 19:37:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n","22/11/22 19:37:32 INFO DAGScheduler: Job 0 finished: count at /content/example.py:5, took 1.720733 s\n","7\n","22/11/22 19:37:32 INFO SparkContext: Invoking stop() from shutdown hook\n","22/11/22 19:37:32 INFO SparkUI: Stopped Spark web UI at http://674afbf15f88:4041\n","22/11/22 19:37:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n","22/11/22 19:37:32 INFO MemoryStore: MemoryStore cleared\n","22/11/22 19:37:32 INFO BlockManager: BlockManager stopped\n","22/11/22 19:37:32 INFO BlockManagerMaster: BlockManagerMaster stopped\n","22/11/22 19:37:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n","22/11/22 19:37:32 INFO SparkContext: Successfully stopped SparkContext\n","22/11/22 19:37:32 INFO ShutdownHookManager: Shutdown hook called\n","22/11/22 19:37:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-0946dbf8-a4b4-42f6-abf1-b46d2f2821e8/pyspark-b5d9286e-e4a7-4b64-a9eb-a75e5d087369\n","22/11/22 19:37:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-0946dbf8-a4b4-42f6-abf1-b46d2f2821e8\n","22/11/22 19:37:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-71affa55-10c4-4897-8c5f-a06862cfe06f\n","\u001b[01;34m/content/spark-3.2.2-bin-hadoop2.7\u001b[00m# ls\n","\u001b[0m\u001b[01;34mbin\u001b[0m   \u001b[01;34mdata\u001b[0m      \u001b[01;34mjars\u001b[0m        LICENSE   NOTICE  \u001b[01;34mR\u001b[0m          RELEASE  \u001b[01;34myarn\u001b[0m\n","\u001b[01;34mconf\u001b[0m  \u001b[01;34mexamples\u001b[0m  \u001b[01;34mkubernetes\u001b[0m  \u001b[01;34mlicenses\u001b[0m  \u001b[01;34mpython\u001b[0m  README.md  \u001b[01;34msbin\u001b[0m\n","\u001b[01;34m/content/spark-3.2.2-bin-hadoop2.7\u001b[00m# \n","\u001b[01;34m/content/spark-3.2.2-bin-hadoop2.7\u001b[00m# \n","\u001b[01;34m/content/spark-3.2.2-bin-hadoop2.7\u001b[00m# exit\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLd9umc5-Ruv"},"outputs":[],"source":["# 필요한 모듈 import\n","from pyspark.ml import Pipeline\n","from pyspark.ml.feature import VectorAssembler, StringIndexer\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.sql import SparkSession\n","from pyspark.sql import DataFrame\n","from pyspark.sql.functions import col\n","from pyspark.sql.types import StructField, StructType"]},{"cell_type":"markdown","metadata":{"id":"vqTPOBmf-pyv"},"source":["# Web UI Setting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOUumZKl-S-X"},"outputs":[],"source":["# ngrok 설치하기\n","!wget -qnc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip -n -q ngrok-stable-linux-amd64.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RWn5NEkr-U-c"},"outputs":[],"source":["get_ipython().system_raw('./ngrok http 4050 &')"]},{"cell_type":"code","source":["import sys \n","sys.path.append('/content/drive/MyDrive/Analysis')\n","from config import config"],"metadata":{"id":"Vlrvs3QwOWlG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1668863983715,"user":{"displayName":"rany go","userId":"15846384859939162308"},"user_tz":-540},"id":"oBT89x00-WAK","outputId":"c2b61224-c94d-4acf-9223-170fcc0359a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"]}],"source":["# ngrok 페이지에 들어가서 내 토큰을 입력한다.\n","!./ngrok authtoken {config.NGROK_TOKEN}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1668863983715,"user":{"displayName":"rany go","userId":"15846384859939162308"},"user_tz":-540},"id":"vy25lCtM-XBp","outputId":"833b6768-f3fa-4fe1-eb8d-284fe7be4838"},"outputs":[{"output_type":"stream","name":"stdout","text":["https://493d-35-196-68-137.ngrok.io\n"]}],"source":["!curl -s http://localhost:4040/api/tunnels | grep -Po 'public_url\":\"(?=https)\\K[^\"]*'"]},{"cell_type":"code","source":[],"metadata":{"id":"ZfdFvAAuOU32"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUwPZZ/LnWhrfTOgqlUH72"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}